{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d2b05a",
   "metadata": {},
   "source": [
    "### Problem 1 (50 points) \n",
    "\n",
    "Vapor-liquid equilibria data are correlated using two adjustable parameters $A_{12}$ and $A_{21}$ per binary\n",
    "mixture. For low pressures, the equilibrium relation can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p = & x_1\\exp\\left(A_{12}\\left(\\frac{A_{21}x_2}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{water}^{sat}\\\\\n",
    "& + x_2\\exp\\left(A_{21}\\left(\\frac{A_{12}x_1}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{1,4 dioxane}^{sat}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here the saturation pressures are given by the Antoine equation\n",
    "\n",
    "$$\n",
    "\\log_{10}(p^{sat}) = a_1 - \\frac{a_2}{T + a_3},\n",
    "$$\n",
    "\n",
    "where $T = 20$($^{\\circ}{\\rm C}$) and $a_{1,2,3}$ for a water - 1,4 dioxane\n",
    "system is given below.\n",
    "\n",
    "|             | $a_1$     | $a_2$      | $a_3$     |\n",
    "|:------------|:--------|:---------|:--------|\n",
    "| Water       | 8.07131 | 1730.63  | 233.426 |\n",
    "| 1,4 dioxane | 7.43155 | 1554.679 | 240.337 |\n",
    "\n",
    "\n",
    "The following table lists the measured data. Recall that in a binary system $x_1 + x_2 = 1$.\n",
    "\n",
    "|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n",
    "|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
    "|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n",
    "\n",
    "Estimate $A_{12}$ and $A_{21}$ using data from the above table: \n",
    "\n",
    "1. Formulate the least square problem; \n",
    "2. Since the model is nonlinear, the problem does not have an analytical solution. Therefore, solve it using the gradient descent or Newton's method implemented in HW1; \n",
    "3. Compare your optimized model with the data. Does your model fit well with the data?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0249338",
   "metadata": {},
   "source": [
    "The least square problem can be formulated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{A_{12},A_{21}}\\sum_{i=1}^{11} (p(x_i;A_{12},A_{21})-p_i)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "It can be aptly described as the sum of the errors squared where the error is defined as the difference between the thoeretical model and the experimental data points. This equation is the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2181965c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A_12 , A_21] =  [1.9584198 1.6891851]\n",
      "Loss =  0.67019004\n",
      "RSE =  0.22339667876561484\n"
     ]
    }
   ],
   "source": [
    "# Here is a code for gradient descent without line search\n",
    "import math\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "\n",
    "A = Variable(t.tensor([1.0, 2.0]), requires_grad=True)\n",
    "\n",
    "# Define constants\n",
    "c1w = 8.07131\n",
    "c2w = 1730.63\n",
    "c3w = 233.426\n",
    "c1d = 7.43155\n",
    "c2d = 1554.679\n",
    "c3d = 240.337\n",
    "\n",
    "# Calculate pressure of saturated water and dixoane at 20 deg C using Antoine Equation and given constants\n",
    "psatwater = 10**(c1w - (c2w/(20+c3w)))\n",
    "psatdioxane = 10**(c1d - (c2d/(20+c3d)))\n",
    "\n",
    "# Create tensors for x1 and x2\n",
    "x1 = t.tensor([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "x2 = 1-x1\n",
    "\n",
    "# Create tensor for p experimental and p theoretical \n",
    "pe = t.tensor([28.1, 34.4, 36.7, 36.9, 36.8, 36.7, 36.5, 35.4, 32.9, 27.7, 17.5])\n",
    "pt = lambda A, x1, x2, psatwater, psatdixoane: x1*t.exp(A[0]*((A[1]*x2)/(A[0]*x1+A[1]*x2))**2)*psatwater + x2*t.exp(A[1]*((A[0]*x1)/(A[0]*x1+A[1]*x2))**2)*psatdioxane \n",
    "\n",
    "\n",
    "# Fix the step size\n",
    "a = 0.001\n",
    "\n",
    "# Start gradient descent\n",
    "for i in range(10000):  # TODO: change the termination criterion\n",
    "    \n",
    "    loss = t.sum(t.square((pt(A,x1,x2,psatwater,psatdioxane)-pe)))\n",
    "    loss.backward()\n",
    "    \n",
    "    # no_grad() specifies that the operations within this context are not part of the computational graph, i.e., we don't need the gradient descent algorithm itself to be differentiable with respect to x\n",
    "    with t.no_grad():\n",
    "        A -= a * A.grad\n",
    "        \n",
    "        # need to clear the gradient at every step, or otherwise it will accumulate...\n",
    "        A.grad.zero_()\n",
    "\n",
    "print('[A_12 , A_21] = ', A.data.numpy())\n",
    "print('Loss = ', loss.data.numpy())\n",
    "print('RSE = ', loss.data.numpy()/3) #Divide loss by square root of number of samples minus degrees of freedom  = sqrt(11 - 2) = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119d5e18",
   "metadata": {},
   "source": [
    "Problem 1.3. The residual standard error is 0.223 which means that 68% of the data points fall within 0.223 mmHg of the model. I would consider the model to fit the data pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-logging",
   "metadata": {},
   "source": [
    "### Problem 2 (50 points) \n",
    "\n",
    "Solve the following problem using Bayesian Optimization:\n",
    "$$\n",
    "    \\min_{x_1, x_2} \\quad \\left(4-2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2,\n",
    "$$\n",
    "for $x_1 \\in [-3,3]$ and $x_2 \\in [-2,2]$. A tutorial on Bayesian Optimization can be found [here](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fe9dc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-1.43478755,  0.71162295],\n",
      "       [ 0.96849286,  1.55848689],\n",
      "       [ 1.890264  , -0.72653126],\n",
      "       [-0.98234029,  1.27431562],\n",
      "       [ 1.12191817,  0.43550978],\n",
      "       [ 0.9503587 ,  1.6056143 ],\n",
      "       [ 0.9073589 ,  1.71832546],\n",
      "       [ 0.84724476,  1.88209045],\n",
      "       [ 0.77848069,  2.        ],\n",
      "       [ 1.61384152,  0.71102915],\n",
      "       [ 2.83499121,  1.5484089 ],\n",
      "       [ 3.        ,  1.81635993],\n",
      "       [ 1.79495492,  0.71384037],\n",
      "       [-0.70149521, -1.50175524],\n",
      "       [-0.3978184 ,  1.21728124],\n",
      "       [ 1.3901811 , -1.45952075],\n",
      "       [-3.        , -2.        ],\n",
      "       [ 2.27878921, -1.90913991],\n",
      "       [-0.26258755, -1.86757578],\n",
      "       [ 1.10778029, -0.98871288],\n",
      "       [ 0.09939043, -1.00645673],\n",
      "       [-2.9416504 ,  0.97310987],\n",
      "       [-1.45299343,  1.63577577],\n",
      "       [-1.62548741, -0.26742491],\n",
      "       [-2.0851698 , -0.17752156],\n",
      "       [-2.69507598,  0.44910906],\n",
      "       [-2.33441516,  0.82153359],\n",
      "       [-1.33207504,  1.06537201],\n",
      "       [-1.54513492,  0.64261115],\n",
      "       [ 1.70879694,  1.73583805],\n",
      "       [ 2.72379014, -1.8173926 ],\n",
      "       [ 0.50178213,  0.81310078],\n",
      "       [-0.57857987,  0.05691848],\n",
      "       [ 0.04418121,  1.50651433],\n",
      "       [-0.47814142, -1.70089908],\n",
      "       [ 2.978242  , -0.81767438],\n",
      "       [-0.21268974, -1.16172713],\n",
      "       [-0.17957167,  1.22684126],\n",
      "       [ 1.43403978, -0.2444313 ],\n",
      "       [ 1.23522385, -1.3811771 ],\n",
      "       [ 0.51268851, -1.13480775],\n",
      "       [-0.43757328, -1.46274266],\n",
      "       [-0.53629108,  0.79675021],\n",
      "       [ 1.41191642,  1.12537842],\n",
      "       [ 0.23317986, -1.17929612],\n",
      "       [-2.91765161, -1.9553681 ],\n",
      "       [ 2.39968574,  1.47448815],\n",
      "       [ 2.50808773, -0.92323827],\n",
      "       [ 1.09555805, -1.39877327],\n",
      "       [-0.13423681, -1.8866606 ],\n",
      "       [ 2.18790122,  0.37549751],\n",
      "       [ 0.19396581, -1.43591601],\n",
      "       [-1.87921583, -1.08030741],\n",
      "       [-0.77873239, -1.01015225],\n",
      "       [-2.70902395, -1.45192462]]), array([ 2.22080556e-01,  1.75711224e+01,  3.17345502e-01,  5.00458115e+00,\n",
      "        2.24627464e+00,  1.99434652e+01,  2.66767461e+01,  3.95286010e+01,\n",
      "        5.12840061e+01,  2.20957136e+00,  8.73464790e+01,  1.44690342e+02,\n",
      "        2.51836398e+00,  1.38769201e+01,  2.95302334e+00,  9.89428986e+00,\n",
      "        1.62900000e+02,  4.50289121e+01,  3.54650707e+01,  1.17916429e+00,\n",
      "       -8.23097448e-03,  9.02880379e+01,  1.77804090e+01,  2.22591875e+00,\n",
      "        5.33884779e+00,  4.41420819e+01,  1.05834067e+01,  1.54181608e+00,\n",
      "        1.53455751e-01,  2.93031453e+01,  7.56825685e+01,  3.91185309e-01,\n",
      "        1.07034675e+00,  1.16000899e+01,  2.35288702e+01,  9.95549504e+01,\n",
      "        2.31111673e+00,  2.94772712e+00,  1.66858758e+00,  7.61817139e+00,\n",
      "        1.81300351e+00,  1.10846543e+01, -3.69940170e-01,  5.20814491e+00,\n",
      "        2.11001471e+00,  1.36385728e+02,  3.07974099e+01,  2.22178561e+01,\n",
      "        8.30600733e+00,  3.67665302e+01,  7.92749459e+00,  8.62657909e+00,\n",
      "        5.42684160e+00,  2.59769299e+00,  6.12817658e+01]))\n"
     ]
    }
   ],
   "source": [
    "# Code is from:\n",
    "# https://github.com/thuijskens/bayesian-optimization\n",
    "\n",
    "\"\"\" gp.py\n",
    "\n",
    "Bayesian optimisation of loss functions.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.gaussian_process as gp\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def expected_improvement(x, gaussian_process, evaluated_loss, greater_is_better=False, n_params=1):\n",
    "    \"\"\" expected_improvement\n",
    "\n",
    "    Expected improvement acquisition function.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        x: array-like, shape = [n_samples, n_hyperparams]\n",
    "            The point for which the expected improvement needs to be computed.\n",
    "        gaussian_process: GaussianProcessRegressor object.\n",
    "            Gaussian process trained on previously evaluated hyperparameters.\n",
    "        evaluated_loss: Numpy array.\n",
    "            Numpy array that contains the values off the loss function for the previously\n",
    "            evaluated hyperparameters.\n",
    "        greater_is_better: Boolean.\n",
    "            Boolean flag that indicates whether the loss function is to be maximised or minimised.\n",
    "        n_params: int.\n",
    "            Dimension of the hyperparameter space.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x_to_predict = x.reshape(-1, n_params)\n",
    "\n",
    "    mu, sigma = gaussian_process.predict(x_to_predict, return_std=True)\n",
    "\n",
    "    if greater_is_better:\n",
    "        loss_optimum = np.max(evaluated_loss)\n",
    "    else:\n",
    "        loss_optimum = np.min(evaluated_loss)\n",
    "\n",
    "    scaling_factor = (-1) ** (not greater_is_better)\n",
    "\n",
    "    # In case sigma equals zero\n",
    "    with np.errstate(divide='ignore'):\n",
    "        Z = scaling_factor * (mu - loss_optimum) / sigma\n",
    "        expected_improvement = scaling_factor * (mu - loss_optimum) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        expected_improvement[sigma == 0.0] == 0.0\n",
    "\n",
    "    return -1 * expected_improvement\n",
    "\n",
    "\n",
    "def sample_next_hyperparameter(acquisition_func, gaussian_process, evaluated_loss, greater_is_better=False,\n",
    "                               bounds=(0, 10), n_restarts=25):\n",
    "    \"\"\" sample_next_hyperparameter\n",
    "\n",
    "    Proposes the next hyperparameter to sample the loss function for.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        acquisition_func: function.\n",
    "            Acquisition function to optimise.\n",
    "        gaussian_process: GaussianProcessRegressor object.\n",
    "            Gaussian process trained on previously evaluated hyperparameters.\n",
    "        evaluated_loss: array-like, shape = [n_obs,]\n",
    "            Numpy array that contains the values off the loss function for the previously\n",
    "            evaluated hyperparameters.\n",
    "        greater_is_better: Boolean.\n",
    "            Boolean flag that indicates whether the loss function is to be maximised or minimised.\n",
    "        bounds: Tuple.\n",
    "            Bounds for the L-BFGS optimiser.\n",
    "        n_restarts: integer.\n",
    "            Number of times to run the minimiser with different starting points.\n",
    "\n",
    "    \"\"\"\n",
    "    best_x = None\n",
    "    best_acquisition_value = 1\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    for starting_point in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, n_params)):\n",
    "\n",
    "        res = minimize(fun=acquisition_func,\n",
    "                       x0=starting_point.reshape(1, -1),\n",
    "                       bounds=bounds,\n",
    "                       method='L-BFGS-B',\n",
    "                       args=(gaussian_process, evaluated_loss, greater_is_better, n_params))\n",
    "\n",
    "        if res.fun < best_acquisition_value:\n",
    "            best_acquisition_value = res.fun\n",
    "            best_x = res.x\n",
    "\n",
    "    return best_x\n",
    "\n",
    "\n",
    "def bayesian_optimisation(n_iters, sample_loss, bounds, x0=None, n_pre_samples=5,\n",
    "                          gp_params=None, random_search=False, alpha=1e-5, epsilon=1e-7):\n",
    "    \"\"\" bayesian_optimisation\n",
    "\n",
    "    Uses Gaussian Processes to optimise the loss function `sample_loss`.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        n_iters: integer.\n",
    "            Number of iterations to run the search algorithm.\n",
    "        sample_loss: function.\n",
    "            Function to be optimised.\n",
    "        bounds: array-like, shape = [n_params, 2].\n",
    "            Lower and upper bounds on the parameters of the function `sample_loss`.\n",
    "        x0: array-like, shape = [n_pre_samples, n_params].\n",
    "            Array of initial points to sample the loss function for. If None, randomly\n",
    "            samples from the loss function.\n",
    "        n_pre_samples: integer.\n",
    "            If x0 is None, samples `n_pre_samples` initial points from the loss function.\n",
    "        gp_params: dictionary.\n",
    "            Dictionary of parameters to pass on to the underlying Gaussian Process.\n",
    "        random_search: integer.\n",
    "            Flag that indicates whether to perform random search or L-BFGS-B optimisation\n",
    "            over the acquisition function.\n",
    "        alpha: double.\n",
    "            Variance of the error term of the GP.\n",
    "        epsilon: double.\n",
    "            Precision tolerance for floats.\n",
    "    \"\"\"\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    if x0 is None:\n",
    "        for params in np.random.uniform(bounds[:, 0], bounds[:, 1], (n_pre_samples, bounds.shape[0])):\n",
    "            x_list.append(params)\n",
    "            y_list.append(sample_loss(params))\n",
    "    else:\n",
    "        for params in x0:\n",
    "            x_list.append(params)\n",
    "            y_list.append(sample_loss(params))\n",
    "\n",
    "    xp = np.array(x_list)\n",
    "    yp = np.array(y_list)\n",
    "\n",
    "    # Create the GP\n",
    "    if gp_params is not None:\n",
    "        model = gp.GaussianProcessRegressor(**gp_params)\n",
    "    else:\n",
    "        kernel = gp.kernels.Matern()\n",
    "        model = gp.GaussianProcessRegressor(kernel=kernel,\n",
    "                                            alpha=alpha,\n",
    "                                            n_restarts_optimizer=10,\n",
    "                                            normalize_y=True)\n",
    "\n",
    "    for n in range(n_iters):\n",
    "\n",
    "        model.fit(xp, yp)\n",
    "\n",
    "        # Sample next hyperparameter\n",
    "        if random_search:\n",
    "            x_random = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(random_search, n_params))\n",
    "            ei = -1 * expected_improvement(x_random, model, yp, greater_is_better=True, n_params=n_params)\n",
    "            next_sample = x_random[np.argmax(ei), :]\n",
    "        else:\n",
    "            next_sample = sample_next_hyperparameter(expected_improvement, model, yp, greater_is_better=True, bounds=bounds, n_restarts=100)\n",
    "\n",
    "        # Duplicates will break the GP. In case of a duplicate, we will randomly sample a next query point.\n",
    "        if np.any(np.abs(next_sample - xp) <= epsilon):\n",
    "            next_sample = np.random.uniform(bounds[:, 0], bounds[:, 1], bounds.shape[0])\n",
    "\n",
    "        # Sample loss for new set of parameters\n",
    "        cv_score = sample_loss(next_sample)\n",
    "\n",
    "        # Update lists\n",
    "        x_list.append(next_sample)\n",
    "        y_list.append(cv_score)\n",
    "\n",
    "        # Update xp and yp\n",
    "        xp = np.array(x_list)\n",
    "        yp = np.array(y_list)\n",
    "\n",
    "    return xp, yp\n",
    "\n",
    "objFun = lambda x: (4-2.1*x[0]**2+(x[0]**4/3))*x[0]**2+x[0]*x[1]+(-4+4*x[1]**2)*x[1]**2\n",
    "bounds = np.array([[-3, 3], [-2, 2]])\n",
    "answer = bayesian_optimisation(50, objFun , bounds, x0=None, n_pre_samples=5, gp_params=None, random_search=False, alpha=1e-5, epsilon=1e-7)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13e7d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb0d04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
