{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "touched-logic",
   "metadata": {},
   "source": [
    "# Theory/Computation Problems\n",
    "\n",
    "### Problem 1 (20 points) \n",
    "Show that the stationary point (zero gradient) of the function\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    f=2x_{1}^{2} - 4x_1 x_2+ 1.5x^{2}_{2}+ x_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "is a saddle (with indefinite Hessian). Find the directions of downslopes away from the saddle. Hint: Use Taylor's expansion at the saddle point. Find directions that reduce $f$.\n",
    "\n",
    "### Problem 2 (50 points) \n",
    "\n",
    "* (10 points) Find the point in the plane $x_1+2x_2+3x_3=1$ in $\\mathbb{R}^3$ that is nearest to the point $(-1,0,1)^T$. Is this a convex problem? Hint: Convert the problem into an unconstrained problem using $x_1+2x_2+3x_3=1$.\n",
    "\n",
    "* (40 points) Implement the gradient descent and Newton's algorithm for solving the problem. Attach your codes along with a short summary including (1) the initial points tested, (2) corresponding solutions, (3) a log-linear convergence plot.\n",
    "\n",
    "### Problem 3 (10 points) \n",
    "Let $f(x)$ and $g(x)$ be two convex functions defined on the convex set $\\mathcal{X}$. \n",
    "* (5 points) Prove that $af(x)+bg(x)$ is convex for $a>0$ and $b>0$. \n",
    "* (5 points) In what conditions will $f(g(x))$ be convex?\n",
    "\n",
    "### Problem 4 (bonus 10 points)\n",
    "Show that $f({\\bf x}_1) \\geq f(\\textbf{x}_0) + \n",
    "    \\textbf{g}_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0)$ for a convex function $f(\\textbf{x}): \\mathcal{X} \\rightarrow \\mathbb{R}$ and for $\\textbf{x}_0$, $\\textbf{x}_1 \\in \\mathcal{X}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-carbon",
   "metadata": {},
   "source": [
    "# Design Problems\n",
    "\n",
    "### Problem 5 (20 points) \n",
    "Consider an illumination problem: There are $n$ lamps and $m$ mirrors fixed to the ground. The target reflection intensity level is $I_t$. The actual reflection intensity level on the $k$th mirror can be computed as $\\textbf{a}_k^T \\textbf{p}$, where $\\textbf{a}_k$ is given by the distances between all lamps to the mirror, and $\\textbf{p}:=[p_1,...,p_n]^T$ are the power output of the lamps. The objective is to keep the actual intensity levels as close to the target as possible by tuning the power output $\\textbf{p}$.\n",
    "\n",
    "* (5 points) Formulate this problem as an optimization problem. \n",
    "* (5 points) Is your problem convex?\n",
    "* (5 points) If we require the overall power output of any of the $n$ lamps to be less than $p^*$, will the problem have a unique solution?\n",
    "* (5 points) If we require no more than half of the lamps to be switched on, will the problem have a unique solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-twins",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "For this homework, you may want to attach sketches as means to explain your ideas. Here is how you can attach images.\n",
    "\n",
    "![everly1](img/everly7.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511227d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2542f154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0],\n",
       " array([ 0.2, -0.2]),\n",
       " array([ 0.36, -0.36]),\n",
       " array([ 0.488, -0.488]),\n",
       " array([ 0.5904, -0.5904]),\n",
       " array([ 0.67232, -0.67232]),\n",
       " array([ 0.737856, -0.737856]),\n",
       " array([ 0.7902848, -0.7902848]),\n",
       " array([ 0.83222784, -0.83222784]),\n",
       " array([ 0.86578227, -0.86578227]),\n",
       " array([ 0.89262582, -0.89262582]),\n",
       " array([ 0.91410065, -0.91410065]),\n",
       " array([ 0.93128052, -0.93128052]),\n",
       " array([ 0.94502442, -0.94502442]),\n",
       " array([ 0.95601953, -0.95601953]),\n",
       " array([ 0.96481563, -0.96481563]),\n",
       " array([ 0.9718525, -0.9718525]),\n",
       " array([ 0.977482, -0.977482]),\n",
       " array([ 0.9819856, -0.9819856]),\n",
       " array([ 0.98558848, -0.98558848]),\n",
       " array([ 0.98847078, -0.98847078]),\n",
       " array([ 0.99077663, -0.99077663]),\n",
       " array([ 0.9926213, -0.9926213]),\n",
       " array([ 0.99409704, -0.99409704]),\n",
       " array([ 0.99527763, -0.99527763]),\n",
       " array([ 0.99622211, -0.99622211]),\n",
       " array([ 0.99697769, -0.99697769]),\n",
       " array([ 0.99758215, -0.99758215]),\n",
       " array([ 0.99806572, -0.99806572]),\n",
       " array([ 0.99845257, -0.99845257]),\n",
       " array([ 0.99876206, -0.99876206]),\n",
       " array([ 0.99900965, -0.99900965]),\n",
       " array([ 0.99920772, -0.99920772]),\n",
       " array([ 0.99936617, -0.99936617]),\n",
       " array([ 0.99949294, -0.99949294]),\n",
       " array([ 0.99959435, -0.99959435]),\n",
       " array([ 0.99967548, -0.99967548])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample code for Problem 2\n",
    "import numpy as np\n",
    "\n",
    "# x[1] corresponds to x2 and x[2] with x3\n",
    "obj = lambda x: (5*x[1]**2+12*x[1]*x[2]-8*x[1]+10*x[2]**2-14*x[2]+5)\n",
    "def grad(x):\n",
    "    return [2*(x[0]-1), 2*(x[1] + 1)]\n",
    "\n",
    "eps = 1e-3  # termination criterion\n",
    "x0 = [0., 0.]  # initial guess\n",
    "k = 0  # counter\n",
    "soln = [x0]  # use an array to store the search steps\n",
    "x = soln[k]  # start with the initial guess\n",
    "\n",
    "error = np.linalg.norm(grad(x))  # compute the error. Note you will need to compute the norm for 2D grads, rather than the absolute value\n",
    "a = 0.1  # set a fixed step size to start with\n",
    "\n",
    "# Armijo line search\n",
    "# def line_search(x):\n",
    "#     a = 1.  # initialize step size\n",
    "#     phi = lambda a, x: obj(x) - a*0.8*grad(x)**2  # define phi as a search criterion\n",
    "#     while phi(a,x)<obj(x-a*grad(x)):  # if f(x+a*d)>phi(a) then backtrack. d is the search direction\n",
    "#         a = 0.5*a\n",
    "#     return a\n",
    "\n",
    "while error >= eps:  # keep searching while gradient norm is larger than eps\n",
    "#     a = line_search(x)\n",
    "    x = np.array(x) - a*np.array(grad(x))\n",
    "    soln.append(x)\n",
    "    error = np.linalg.norm(grad(x))\n",
    "soln  # print the search trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b10b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
